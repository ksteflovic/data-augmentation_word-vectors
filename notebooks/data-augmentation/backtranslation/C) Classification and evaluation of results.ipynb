{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:01:02.205644Z",
     "end_time": "2023-04-22T03:01:04.500664Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stefi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "from statistics import mean, median\n",
    "\n",
    "import en_core_web_sm\n",
    "import gensim.utils\n",
    "import gensim.models.keyedvectors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import MinMaxScaler # for multinomial\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(format=\"%(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcia na predspracovanie textu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:01:04.506649Z",
     "end_time": "2023-04-22T03:01:04.517647Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    try:\n",
    "        # Prevediem písmená slova na malé písmená\n",
    "        text = text.lower()\n",
    "\n",
    "        # Odfiltrujem text\n",
    "        text = re.sub(r\"(@\\[A-Za-zÀ-ž0-9]+)|([^0-9A-Za-zÀ-ž \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "        # Odstránim slová, ktoré obsahujú čísla\n",
    "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "        # Odstránim čísla samotné\n",
    "        text = re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "        # Odstránim všetky biele znaky\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Prevediem text vety\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        # Ak sú nejaké prázdne stringy, tak ich odstránim\n",
    "        text = [x for x in text if x!='']\n",
    "\n",
    "        # Odstránim slov slová.\n",
    "        text = [x for x in text if x not in stop_words]\n",
    "\n",
    "        # Zlemmatizujem.\n",
    "        text = [lemmatizer.lemmatize(x) for x in text]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Načítanie slovných vektorov (potrebné C) 5.3 krok - vektory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6* Netreba zabudnúť, že pri úprave klasifikačného datasetu sa využívajú slovné vektory, preto všetky ďalšie kroky treba VŽDY spustiť, ak meníme model vektorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:01:04.521609Z",
     "end_time": "2023-04-22T03:01:04.555521Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tu treba zmeniť konkrétny model, kde sú uložené slovné vektory\n",
    "#Vektory pre NEUPRAVENÉ VETY = 0\n",
    "#Vektory pre UPRAVENÉ VETY (o synonymá) = 1\n",
    "\n",
    "file = 0\n",
    "if file == 0:\n",
    "    file = \"model_SKIPGRAM_dim_150_202304211745_sentences\"\n",
    "    sentence_type = \"sentences\"\n",
    "else:\n",
    "    file = \"model_SKIPGRAM_dim_150_202304211745_sentences_new\"\n",
    "    sentence_type = \"sentences_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:01:04.534575Z",
     "end_time": "2023-04-22T03:01:04.722580Z"
    }
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"files/\"+file+\".bin\", binary=True, unicode_errors='replace')\n",
    "\n",
    "# načítanie slovných vektorov z modelu\n",
    "embedding_matrix = model.vectors\n",
    "embedding_size = embedding_matrix.shape[1]\n",
    "word_index = model.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Načítanie datasetu FakeNews a jeho predspracovanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Očistenie a spočítanie slov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:01:49.511995Z",
     "end_time": "2023-04-22T03:06:02.018325Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification\n",
    "# Stačí raz zbehnúť\n",
    "df = pd.read_csv(\"../../../data/WELFake_Dataset.csv\")\n",
    "\n",
    "# odstránenie zbytočných štĺpcov a duplikátov\n",
    "df.drop(labels = [\"Unnamed: 0\", \"title\"], axis = 1, inplace=True)\n",
    "df.drop_duplicates(inplace = True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# očistenie textu a odstránenie NaN záznamov a spočítanie slov\n",
    "df['text_clean'] = df['text'].apply(lambda x: text_preprocessing(x))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.dropna(subset=['text_clean'], how='any')\n",
    "df['word_count'] = df['text_clean'].apply(len)\n",
    "\n",
    "# odstránenie stlpcu text a záznamov, ktoré obsahujú 0 slov\n",
    "df.drop(labels = [\"text\"], axis = 1, inplace=True)\n",
    "df = df.loc[df['word_count'] != 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.to_csv('files/fake_news_clean.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Načítanie očisteného a čistenie na základe slovných vektorov (potrebné D) 6. krok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:02.023311Z",
     "end_time": "2023-04-22T03:06:38.992794Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"files/fake_news_clean.csv\")\n",
    "\n",
    "#očistím text od slov, ktoré sa nevyskytujú v slovníku vektorizovaných slov\n",
    "word_index_set = set(word_index.keys())\n",
    "df['filtered_text'] = df['text_clean'].apply(lambda x: [word for word in eval(x) if word in word_index_set])\n",
    "\n",
    "#spočítam počty slov po očistení\n",
    "df['word_filter_count'] = df['filtered_text'].apply(len)\n",
    "\n",
    "#odstránenie záznamov, ktoré obsahujú 0 slov\n",
    "df = df.loc[df['word_filter_count'] != 0]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.to_csv('files/fake_news_filtered_'+sentence_type+'.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Dátový súbor približne rovnakej veľkosti z hladiska počtu slov pre záznam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:38.995786Z",
     "end_time": "2023-04-22T03:06:42.547054Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"files/fake_news_filtered_\"+sentence_type+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:42.549049Z",
     "end_time": "2023-04-22T03:06:42.655275Z"
    }
   },
   "outputs": [],
   "source": [
    "#odstránime 5% extrémnych hodnot zhora a zdola\n",
    "\n",
    "pocty_prvkov = df['filtered_text'].apply(len)\n",
    "# odstránenie extrémnych hodnôt\n",
    "data_clipped = np.clip(pocty_prvkov, np.percentile(pocty_prvkov, 5), np.percentile(pocty_prvkov, 95))\n",
    "\n",
    "# výpočet percentilových hodnôt\n",
    "percentil_5 = np.percentile(data_clipped, 5)\n",
    "percentil_95 = np.percentile(data_clipped, 95)\n",
    "\n",
    "df = df[(df['filtered_text'].apply(len) >= percentil_5) & (df['filtered_text'].apply(len) <= percentil_95)]\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Vytvorenie sekvencií zo slovných vektorov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8* Hodnota premennej  max_len_seq bude obsahovať počet slov v každom príspevku, ktorý budeme klasifikovať, čím vyššia je, tým kvalitnejší výstup dostaneme, pravdepodobne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:42.656273Z",
     "end_time": "2023-04-22T03:06:42.671232Z"
    }
   },
   "outputs": [],
   "source": [
    "#Počet slov z každého článku, ktoré sa transformujú na vektory a budú tvoriť sekvenciu\n",
    "max_len_seq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:42.747029Z",
     "end_time": "2023-04-22T03:06:59.440289Z"
    }
   },
   "outputs": [],
   "source": [
    "# vytvorenie sekvencií slov\n",
    "sequences = []\n",
    "for text in df['filtered_text']:\n",
    "    seq = []\n",
    "    counter = 0\n",
    "    for word in eval(text):\n",
    "        seq.append(embedding_matrix[word_index[word]])\n",
    "        counter += 1\n",
    "        if max_len_seq <= counter:\n",
    "            break\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:59.444278Z",
     "end_time": "2023-04-22T03:06:59.456247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "56298"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:59.457244Z",
     "end_time": "2023-04-22T03:06:59.981913Z"
    }
   },
   "outputs": [],
   "source": [
    "# vyrovnávanie dĺžky sekvencií na maximálnu dĺžku\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=max_len_seq, dtype=np.float32)\n",
    "padded_sequences = np.array(padded_sequences).reshape(len(padded_sequences), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:06:59.983908Z",
     "end_time": "2023-04-22T03:07:00.568652Z"
    }
   },
   "outputs": [],
   "source": [
    "#Uloženie sekvencií vektorov jednotlivých slov pre záznamy\n",
    "with open(\"files/padded_sequences_\"+sentence_type, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(padded_sequences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:07:00.571644Z",
     "end_time": "2023-04-22T03:07:00.582614Z"
    }
   },
   "outputs": [],
   "source": [
    "#Uloženie labels\n",
    "with open(\"files/label_\"+sentence_type, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(df['label'], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Klasifikácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Načítanie sekvencií slovných vektorov (potrebné D) 8. krok) a label pre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:07:00.584609Z",
     "end_time": "2023-04-22T03:07:00.890790Z"
    }
   },
   "outputs": [],
   "source": [
    "#Načítanie sekvencií vektorov jednotlivých slov pre záznamy\n",
    "with open(\"files/padded_sequences_\"+sentence_type, \"rb\") as fp:   # Unpickling\n",
    "    padded_sequences = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:07:00.891788Z",
     "end_time": "2023-04-22T03:07:00.906747Z"
    }
   },
   "outputs": [],
   "source": [
    "#Načítanie labels\n",
    "with open(\"files/label_\"+sentence_type, \"rb\") as fp:   # Unpickling\n",
    "    labels = pd.read_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Vyváženie datasetu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:07:00.907745Z",
     "end_time": "2023-04-22T03:07:00.925697Z"
    }
   },
   "outputs": [],
   "source": [
    "X = padded_sequences\n",
    "y = labels\n",
    "X = X[:15000]\n",
    "y = y[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-22T03:07:00.923702Z",
     "end_time": "2023-04-22T03:07:00.986534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7269\n",
      "1    7269\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X, y = RandomUnderSampler(random_state=42).fit_resample(X, y)\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 K-násobná stratifikovaná krížová validácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T03:07:00.994513Z",
     "end_time": "2023-04-22T03:45:55.049763Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefi\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for: model_SKIPGRAM_dim_150_202304211745_sentences with classifier: LinearSVC()\n",
      "Average accuracy: 81.74%\n",
      "Average precision: 81.06%\n",
      "Average recall: 82.86%\n",
      "Average F1 score: 81.94%\n",
      "KNeighborsClassifier()\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Results for: model_SKIPGRAM_dim_150_202304211745_sentences with classifier: KNeighborsClassifier()\n",
      "Average accuracy: 75.26%\n",
      "Average precision: 77.07%\n",
      "Average recall: 71.95%\n",
      "Average F1 score: 74.38%\n",
      "AdaBoostClassifier(learning_rate=1)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Results for: model_SKIPGRAM_dim_150_202304211745_sentences with classifier: AdaBoostClassifier(learning_rate=1)\n",
      "Average accuracy: 82.49%\n",
      "Average precision: 79.61%\n",
      "Average recall: 87.40%\n",
      "Average F1 score: 83.31%\n",
      "BernoulliNB()\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Results for: model_SKIPGRAM_dim_150_202304211745_sentences with classifier: BernoulliNB()\n",
      "Average accuracy: 79.37%\n",
      "Average precision: 74.77%\n",
      "Average recall: 88.71%\n",
      "Average F1 score: 81.13%\n",
      "MultinomialNB()\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Results for: model_SKIPGRAM_dim_150_202304211745_sentences with classifier: MultinomialNB()\n",
      "Average accuracy: 76.67%\n",
      "Average precision: 74.11%\n",
      "Average recall: 82.03%\n",
      "Average F1 score: 77.86%\n",
      "SVC()\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "               # RandomForestClassifier(n_estimators=100),\n",
    "               # LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=200),\n",
    "               # SGDClassifier(),\n",
    "               LinearSVC(),\n",
    "               KNeighborsClassifier(n_neighbors=5),\n",
    "               AdaBoostClassifier(n_estimators=50, learning_rate=1),\n",
    "               BernoulliNB(),\n",
    "               MultinomialNB(),\n",
    "               svm.SVC()]\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(str(classifier))\n",
    "    \n",
    "    pocet = 0\n",
    "    \n",
    "    list_acc = []\n",
    "    list_pre = []\n",
    "    list_rec = []\n",
    "    list_f1 = []\n",
    "    list_matrix = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        pocet += 1\n",
    "        print(pocet)\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        if(str(classifier) == \"MultinomialNB()\"):\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        else:\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        list_acc.append(accuracy_score(y_test, y_pred))\n",
    "        list_pre.append(precision_score(y_test, y_pred))\n",
    "        list_rec.append(recall_score(y_test, y_pred))\n",
    "        list_f1.append(f1_score(y_test, y_pred))\n",
    "        list_matrix.append(confusion_matrix(y_test, y_pred))\n",
    "         \n",
    "    print(\"Results for: \"+ file+ \" with classifier: \"+ str(classifier))\n",
    "    print(\"Average accuracy: {:.2f}%\".format(np.mean(list_acc)*100))\n",
    "    print(\"Average precision: {:.2f}%\".format(np.mean(list_pre)*100))\n",
    "    print(\"Average recall: {:.2f}%\".format(np.mean(list_rec)*100))\n",
    "    print(\"Average F1 score: {:.2f}%\".format(np.mean(list_f1)*100))\n",
    "\n",
    "    # cmap = sns.diverging_palette(250, 10, s=80, l=55, n=9, as_cmap=True)\n",
    "    # plt.subplots(figsize=(6,5))\n",
    "    # sns.heatmap((list_matrix[0] + list_matrix[1]), cmap=cmap, annot=True, annot_kws={'size': 15}, fmt='g')\n",
    "    #\n",
    "    # plt.ylabel('Actual')\n",
    "    # plt.xlabel('Predicted')\n",
    "    # plt.title(\"Confusion Matrix for \"+ str(classifier));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove dim 200, 10k, 10 words, neupravené\n",
    "# Average accuracy: 75.00%\n",
    "# Average precision: 73.44%\n",
    "# Average recall: 78.36%\n",
    "# Average F1 score: 75.81%\n",
    "\n",
    "\n",
    "\n",
    "# dim=200, 50k sentences, 20 words per dataset\n",
    "#Upravené vety: \n",
    "# Average accuracy: 79.55%\n",
    "# Average precision: 80.18%\n",
    "# Average recall: 78.52%\n",
    "# Average F1 score: 79.33%\n",
    "\n",
    "#Neupravené vety: \n",
    "# Average accuracy: 82.99%\n",
    "# Average precision: 81.79%\n",
    "# Average recall: 84.90%\n",
    "# Average F1 score: 83.31%\n",
    "\n",
    "\n",
    "\n",
    "# dim=200, 10k sentences, 10 words per dataset\n",
    "#Upravené vety: \n",
    "# Average accuracy: 83.80%\n",
    "# Average precision: 81.78%\n",
    "# Average recall: 86.99%\n",
    "# Average F1 score: 84.30%\n",
    "\n",
    "#Neupravené vety: \n",
    "# Average accuracy: 81.28%\n",
    "# Average precision: 78.03%\n",
    "# Average recall: 87.08%\n",
    "# Average F1 score: 82.31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "# pocet = 0\n",
    "#\n",
    "# skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "#\n",
    "# list_acc = []\n",
    "# list_pre = []\n",
    "# list_rec = []\n",
    "# list_f1 = []\n",
    "# list_matrix = []\n",
    "#\n",
    "# for train_index, test_index in skf.split(X, y):\n",
    "#     pocet+=1\n",
    "#     print(pocet)\n",
    "#\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#\n",
    "#     model.fit(X_train, y_train)\n",
    "#\n",
    "#     y_pred = model.predict(X_test)\n",
    "#\n",
    "#     list_acc.append(accuracy_score(y_test, y_pred))\n",
    "#     list_pre.append(precision_score(y_test, y_pred))\n",
    "#     list_rec.append(recall_score(y_test, y_pred))\n",
    "#     list_f1.append(f1_score(y_test, y_pred))\n",
    "#     list_matrix.append(confusion_matrix(y_test, y_pred))\n",
    "#\n",
    "# print(\"Average accuracy: {:.2f}%\".format(np.mean(list_acc)*100))\n",
    "# print(\"Average precision: {:.2f}%\".format(np.mean(list_pre)*100))\n",
    "# print(\"Average recall: {:.2f}%\".format(np.mean(list_rec)*100))\n",
    "# print(\"Average F1 score: {:.2f}%\".format(np.mean(list_f1)*100))\n",
    "#\n",
    "# cmap = sns.diverging_palette(250, 10, s=80, l=55, n=9, as_cmap=True)\n",
    "# plt.subplots(figsize=(6,5))\n",
    "# sns.heatmap((list_matrix[0] + list_matrix[1]), cmap=cmap, annot=True, annot_kws={'size': 15}, fmt='g')\n",
    "#\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.title('Confusion Matrix');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
