{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "from statistics import mean, median\n",
    "\n",
    "import en_core_web_sm\n",
    "import gensim.utils\n",
    "import gensim.models.keyedvectors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(format=\"%(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):  \n",
    "    # Prevediem písmená slova na malé písmená\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Odfiltrujem text\n",
    "    text = re.sub(r\"(@\\[A-Za-zÀ-ž0-9]+)|([^0-9A-Za-zÀ-ž \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    \n",
    "    # Odstránim slová, ktoré obsahujú čísla\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Odstránim čísla samotné\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "    # Odstránim všetky biele znaky\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Prevediem text vety  \n",
    "    text = word_tokenize(text)\n",
    "        \n",
    "    # Ak sú nejaké prázdne stringy, tak ich odstránim\n",
    "    text = [x for x in text if x!='']\n",
    "\n",
    "    # Odstránim slov slová.\n",
    "    text = [x for x in text if x not in stop_words]\n",
    "    \n",
    "    # Zlemmatizujem.\n",
    "    text = [lemmatizer.lemmatize(x) for x in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Výber stĺpca content a uloženie do súboru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/anthonyc1/gathering-real-news-for-oct-dec-2016/data\n",
    "df = pd.read_csv(\"files/real_news.csv\")\n",
    "df[\"content\"].to_csv('files/real_news.txt', index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Predspracovanie textu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = open('files/real_news.txt',\"r\",encoding='utf-8').read().replace(\"\\n\",\" \")\n",
    "lines = nltk.sent_tokenize(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for line in lines:\n",
    "    line = text_preprocessing(line)\n",
    "    #Ak nie je prázdny výstup ([]), tak ho pridaj\n",
    "    if line:\n",
    "        sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Počet viet: 704357\n"
     ]
    }
   ],
   "source": [
    "print(\"Počet viet:\",len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Náhodne vyberie 50 000 viet z celkového počtu viet\n",
    "# import random\n",
    "# random_sample = random.sample(sentences, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uloženie viet\n",
    "with open(\"files/sentences\", \"wb\") as fp:   #uložím len 10k viet\n",
    "    pickle.dump(sentences[:10000], fp)\n",
    "    \n",
    "# with open(\"files/sentences\", \"wb\") as fp:\n",
    "#     pickle.dump(random_sample, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Duplikácia viet, náhrada slov jeho synonymami (potrebujem synonymá B) 3. krok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Načítanie viet\n",
    "with open(\"files/sentences\", \"rb\") as fp:   # Unpickling\n",
    "    sentences = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Načítanie zoznamu synoným\n",
    "with open(\"files/synonyms\", \"rb\") as fp:   # Unpickling\n",
    "    syn = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonyms(sentences, synonyms):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word in synonyms:\n",
    "                for syn in synonyms[word]:\n",
    "                    new_sentence = sentence.copy()\n",
    "                    new_sentence[i] = syn\n",
    "                    new_sentences.append(new_sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_new = replace_synonyms(sentences,syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 175354\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(sentences_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uloženie viet\n",
    "with open(\"files/sentences_new\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(sentences_new, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
