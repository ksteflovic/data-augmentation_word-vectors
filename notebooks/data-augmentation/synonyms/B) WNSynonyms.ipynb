{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "from statistics import mean, median\n",
    "\n",
    "import en_core_web_sm\n",
    "import gensim.utils\n",
    "import gensim.models.keyedvectors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(format=\"%(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_sim(syn1, syn2, lemma, sim_type='path'):\n",
    "    if sim_type == \"path\":\n",
    "        sim = syn1.path_similarity(syn2)\n",
    "    elif sim_type == \"wup\":\n",
    "        sim = syn1.wup_similarity(syn2)\n",
    "    elif sim_type == \"lch\":\n",
    "        sim = syn1.lch_similarity(syn2)     \n",
    "    else:\n",
    "        raise Exception(\"'{}' is not a valid similarity type.\".format(sim_type))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(word):\n",
    "    # POS tagging\n",
    "    global tok\n",
    "\n",
    "    synonyms = [word]\n",
    "    doc = nlp(word)\n",
    "    \n",
    "    for tok in doc:\n",
    "        synsets_ = []\n",
    "        \n",
    "        if tok.pos_ in (\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PROPN\"):\n",
    "            \n",
    "            try:\n",
    "                synsets_ = wordnet.synsets(word)\n",
    "            \n",
    "                if len(synsets_) == 0:\n",
    "                    # try with Morphy. e.g.: 'denied' -> 'deny'\n",
    "                    word_morphed = wordnet.morphy(word)\n",
    "                    if word_morphed is None:\n",
    "                        return synonyms\n",
    "                    synsets_ = wordnet.synsets(word_morphed)\n",
    "            except Exception as e:\n",
    "                logger.error(\"Error retrieving synsets for {} as a '{}': {}\".format(word, tok.pos_, e))\n",
    "                logger.exception(e)\n",
    "\n",
    "        try:\n",
    "            for syn in synsets_:\n",
    "                for lemma in syn.lemmas():\n",
    "                    sim = wordnet_sim(syn, synsets_[0], lemma)\n",
    "                    if sim and sim > 0.5:\n",
    "                        if (lemma.name()).lower() not in synonyms:\n",
    "                            synonyms.append((lemma.name()).lower())\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error while looking for synonyms: {}\".format(e))\n",
    "            logger.exception(e)\n",
    "\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = search(word)\n",
    "    \n",
    "    word = synonyms[0]\n",
    "    if len(synonyms) > 1 and (word not in synonyms_all):\n",
    "        synonyms_all[word] = synonyms[1:6]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Načítanie upravených viet a vytvorenie zoznamu synoným zo synsetov (potrebujem vety A) 2. krok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Načítanie viet\n",
    "import pickle\n",
    "with open(\"files/sentences\", \"rb\") as fp:   # Unpickling\n",
    "    sentences = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 10000\n",
      "1000 of 10000\n",
      "2000 of 10000\n",
      "3000 of 10000\n",
      "4000 of 10000\n",
      "5000 of 10000\n",
      "6000 of 10000\n",
      "7000 of 10000\n",
      "8000 of 10000\n",
      "9000 of 10000\n"
     ]
    }
   ],
   "source": [
    "synonyms_all = dict()\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        get_synonyms(sentences[i][j])\n",
    "    if i % 1000 == 0:\n",
    "        print(i, \"of\", len(sentences))\n",
    "        \n",
    "        \n",
    "with open(\"files/synonyms\", \"wb\") as fp:\n",
    "    pickle.dump(synonyms_all, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words for which synonyms were found:  9704\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of words for which synonyms were found: \",len(synonyms_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
